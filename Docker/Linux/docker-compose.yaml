# System-level configuration are the same for all develop services
x-common-config: &common-config
  hostname: ${HOSTNAME:-macslam}-dev
  volumes:
    - ../../:/workspace
  working_dir: /workspace
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: all
            capabilities: [gpu]
  stdin_open: true
  runtime: nvidia
  tty: true

  # For profiliing in a Docker container
  cap_add:
    - SYS_ADMIN
  privileged: true

  # Sufficient IPC shared memory for PyTorch as instructed in NVIDIA container.
  ipc: host
  ulimits:
    memlock:
      soft: -1
      hard: -1
    stack: 67108864

  # Auxilary environment variables
  environment:
    - NVIDIA_DRIVER_CAPABILITIES=all
    - NVIDIA_VISIBLE_DEVICES=all
    - PYTHONUNBUFFERED=1

# Per-resource configuration service.
services:
  linux-cuda12-dev:
    <<: *common-config
    profiles: ["amd64"]
    platform: linux/amd64
    image: macslam/warp-pypose:linux-cu128-amd64
    build:
      context: .
      dockerfile: Dockerfile
      args:
        BASE_IMAGE: "nvcr.io/nvidia/pytorch:25.03-py3"
        CUDA_MAJOR_VERSION: "12"


  linux-cuda13-dev:
    <<: *common-config
    profiles: ["amd64"]
    platform: linux/amd64
    image: macslam/warp-pypose:linux-cu130-amd64
    build:
      context: .
      dockerfile: Dockerfile
      args:
        BASE_IMAGE: "nvcr.io/nvidia/pytorch:25.08-py3"
        CUDA_MAJOR_VERSION: "13"

  jetson-thor-dev:
    <<: *common-config
    profiles: ["arm64"]
    platform: linux/arm64
    image: macslam/warp-pypose:linux-cu130-thor-arm64
    build:
      context: .
      dockerfile: Dockerfile
      args:
        BASE_IMAGE: "nvcr.io/nvidia/pytorch:25.08-py3"
        CUDA_MAJOR_VERSION: "13"
    
    # Load stripped nsys module.
    environment:
      - PATH=/opt/nvidia/nsight-systems/2025.4.1/bin:/opt/nvidia/nsight-compute/2025.3.0/bin:$PATH
    
    volumes:
      - ../../:/workspace
      - /opt/nvidia/nsight-systems/:/opt/nvidia/nsight-systems/
      - /opt/nvidia/nsight-compute/:/opt/nvidia/nsight-compute/
  
  jetson-orin-dev:
    <<: *common-config
    profiles: ["arm64"]
    platform: linux/arm64
    image: macslam/warp-pypose:linux-cu126-orin-arm64
    build:
      context: .
      dockerfile: Dockerfile
      args:
        BASE_IMAGE: "nvcr.io/nvidia/pytorch:25.03-py3-igpu"
        CUDA_MAJOR_VERSION: "12"

    # Fix the libjpeg library problem for PyZed & Load stripped nsys
    environment:
      - PATH=/opt/nvidia/nsight-systems/2024.5.4/bin:/opt/nvidia/nsight-compute/2024.3.1/bin:$PATH
    
    volumes:
      - ../../:/workspace
      - /usr/lib/aarch64-linux-gnu/tegra:/usr/lib/aarch64-linux-gnu/tegra
      - /usr/lib/aarch64-linux-gnu/libjpeg.so.8:/usr/lib/aarch64-linux-gnu/libjpeg.so.8
      - /opt/nvidia/nsight-systems/:/opt/nvidia/nsight-systems/
      - /opt/nvidia/nsight-compute/:/opt/nvidia/nsight-compute/

  test-type-jetson:
    profiles: ["arm64"]
    platform: linux/arm64
    image: macslam/warp-pypose:linux-static-analysis-arm64
    working_dir: /workspace
    
    build:
      context: .
      dockerfile: Dockerfile.test-type
      args:
        BASE_IMAGE: macslam/warp-pypose:linux-cu126-orin-arm64
    
    command: "pyright"
    volumes:
      - ../../:/workspace

  test-cfg-jetson:
    <<: *common-config
    profiles: ["arm64"]
    platform: linux/arm64
    image: macslam/warp-pypose:linux-cu126-orin-arm64
    command: "pytest -m 'not local'"

  test-type:
    profiles: ["amd64"]
    platform: linux/amd64
    image: macslam/warp-pypose:linux-static-analysis-amd64
    working_dir: /workspace
    
    build:
      context: .
      dockerfile: Dockerfile.test-type
      args:
        BASE_IMAGE: "macslam/warp-pypose:linux-cu128-amd64"
    
    volumes:
      - ../../:/workspace

  test-cfg:
    <<: *common-config
    profiles: ["amd64"]
    platform: linux/amd64
    image: macslam/warp-pypose:linux-cu128-amd64
    command: "pytest -m 'not local'"

  dev-unified-entry:
    <<: *common-config

    image: ${DEV_IMAGE}
    platform: ${DEV_PLATFORM}
    container_name: warp-pypose-unified-dev
    command: sleep infinity
